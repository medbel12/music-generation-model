## Music Generation Model with music21, LSTM & Attention Mechanism
This repository contains a deep learning-based music generation model that uses an LSTM (Long Short-Term Memory) network combined with an attention mechanism to generate musical compositions. The model is implemented using the music21 library for music data manipulation and preprocessing.

Features
LSTM-based Model: Utilizes LSTM layers to model sequential dependencies in musical data.
Attention Mechanism: Enhances the LSTM by allowing the model to focus on relevant parts of the sequence while generating music.
Music Representation: Uses music21 for representing musical notes and structures (e.g., pitch, duration, dynamics).
Customizable Outputs: Generate music in various formats (MIDI, MusicXML, etc.).
Prerequisites
Python 3.x
TensorFlow 
music21
numpy
matplotlib (for visualizations)
scikit-learn (for data preprocessing)
